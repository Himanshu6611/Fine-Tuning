# LLM Fine-Tuning with Shoolini Dataset

This repository contains my work on **Supervised Fine-Tuning (SFT)** of a pre-trained large language model using a custom academic dataset **`shoolini.txt`**.  
The goal of this project is to adapt a general NLP model to better understand and generate responses related to **university, educational, and technical content**.

---

## ğŸ“š Dataset

**`shoolini.txt`** is a curated domain-specific dataset created for this project.  
It includes academic notes, technical explanations, project references, and other educational text relevant to:

- University coursework  
- Software engineering fundamentals  
- AI & Machine Learning concepts  
- Research material and summaries  

The dataset was cleaned and formatted before training and used to build a dedicated fine-tuning corpus.

---

## ğŸ§  Training Setup

- **Base Model:** GPT-2  
- **Training Method:** Supervised Fine-Tuning (SFT)  
- **Framework:** HuggingFace Transformers + PyTorch  

---

## ğŸ” Training Workflow

Dataset Preparation  
â†’ Tokenization  
â†’ Model Fine-Tuning  
â†’ Checkpoint Saving  
â†’ Inference Testing

---

## ğŸ§ª Skills Gained

- Creation and preprocessing of custom NLP datasets  
- LLM supervised fine-tuning techniques  
- Managing training checkpoints and evaluations  
- Understanding modern NLP training pipelines  
- Open-source project documentation & version control

---

## ğŸ“ Project Structure

â”œâ”€â”€ data/

â”‚ â””â”€â”€ shoolini.txt


â”œâ”€â”€ fine_tune_gpt2.py

â”œâ”€â”€ generate.py

â”œâ”€â”€ outputs/

â”‚ â””â”€â”€ trained_model/

â””â”€â”€ README.md

```bash

Start Fine-Tuning
python fine_tune_gpt2.py

Test the Trained Model
python generate.py
